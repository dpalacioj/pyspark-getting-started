{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNwat2RyXLo7irfdDLQ/LJg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dpalacioj/pyspark-getting-started/blob/main/PySpark_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Upload Drive and Spark System\n",
        "\n"
      ],
      "metadata": {
        "id": "MAnPeRldHFob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJHRs1oQw1dy",
        "outputId": "b933fdc4-b959-47ec-8c42-7ac16dd11aea"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRIi2yKwt_Mt",
        "outputId": "b3906119-c02e-481e-c0b0-2c731e06171a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 339 ms, sys: 41.9 ms, total: 381 ms\n",
            "Wall time: 44.2 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.1-bin-hadoop3.tgz\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Enviroment Variables\n",
        "\n"
      ],
      "metadata": {
        "id": "cCuSwOGWuU2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.1-bin-hadoop3\""
      ],
      "metadata": {
        "id": "ZB-cQZYkuFW8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVdJKPnEudaF",
        "outputId": "d8755fa1-a706-4088-cc02-7b9f99f559b5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  sample_data  spark-3.5.1-bin-hadoop3  spark-3.5.1-bin-hadoop3.tgz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate() # Always to create a Session\n",
        "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # Property used to format output tables better\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "uSgQmAEyueEg",
        "outputId": "6e1f4100-2ce7-4d72-baf7-161810b820b2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7e593f2e0a60>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://c4c2cc199fa6:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Uploading Data"
      ],
      "metadata": {
        "id": "iVqtVB1TKHz9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = r\"/content/drive/MyDrive/projects/spark-learn/tips.csv\""
      ],
      "metadata": {
        "id": "5kyij8gd-aXC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_pyspark=spark.read.csv(path)"
      ],
      "metadata": {
        "id": "0lcJv_eOuwyt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "-io-YkS4uy_z",
        "outputId": "b63be29e-bdda-457e-8172-bc3ca2cadf33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "+----------+----+------+------+---+------+----+\n",
              "|       _c0| _c1|   _c2|   _c3|_c4|   _c5| _c6|\n",
              "+----------+----+------+------+---+------+----+\n",
              "|total_bill| tip|   sex|smoker|day|  time|size|\n",
              "|     16.99|1.01|Female|    No|Sun|Dinner|   2|\n",
              "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|\n",
              "|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|\n",
              "|     23.68|3.31|  Male|    No|Sun|Dinner|   2|\n",
              "|     24.59|3.61|Female|    No|Sun|Dinner|   4|\n",
              "|     25.29|4.71|  Male|    No|Sun|Dinner|   4|\n",
              "|      8.77| 2.0|  Male|    No|Sun|Dinner|   2|\n",
              "|     26.88|3.12|  Male|    No|Sun|Dinner|   4|\n",
              "|     15.04|1.96|  Male|    No|Sun|Dinner|   2|\n",
              "|     14.78|3.23|  Male|    No|Sun|Dinner|   2|\n",
              "|     10.27|1.71|  Male|    No|Sun|Dinner|   2|\n",
              "|     35.26| 5.0|Female|    No|Sun|Dinner|   4|\n",
              "|     15.42|1.57|  Male|    No|Sun|Dinner|   2|\n",
              "|     18.43| 3.0|  Male|    No|Sun|Dinner|   4|\n",
              "|     14.83|3.02|Female|    No|Sun|Dinner|   2|\n",
              "|     21.58|3.92|  Male|    No|Sun|Dinner|   2|\n",
              "|     10.33|1.67|Female|    No|Sun|Dinner|   3|\n",
              "|     16.29|3.71|  Male|    No|Sun|Dinner|   3|\n",
              "|     16.97| 3.5|Female|    No|Sun|Dinner|   3|\n",
              "+----------+----+------+------+---+------+----+\n",
              "only showing top 20 rows"
            ],
            "text/html": [
              "<table border='1'>\n",
              "<tr><th>_c0</th><th>_c1</th><th>_c2</th><th>_c3</th><th>_c4</th><th>_c5</th><th>_c6</th></tr>\n",
              "<tr><td>total_bill</td><td>tip</td><td>sex</td><td>smoker</td><td>day</td><td>time</td><td>size</td></tr>\n",
              "<tr><td>16.99</td><td>1.01</td><td>Female</td><td>No</td><td>Sun</td><td>Dinner</td><td>2</td></tr>\n",
              "<tr><td>10.34</td><td>1.66</td><td>Male</td><td>No</td><td>Sun</td><td>Dinner</td><td>3</td></tr>\n",
              "<tr><td>21.01</td><td>3.5</td><td>Male</td><td>No</td><td>Sun</td><td>Dinner</td><td>3</td></tr>\n",
              "<tr><td>23.68</td><td>3.31</td><td>Male</td><td>No</td><td>Sun</td><td>Dinner</td><td>2</td></tr>\n",
              "<tr><td>24.59</td><td>3.61</td><td>Female</td><td>No</td><td>Sun</td><td>Dinner</td><td>4</td></tr>\n",
              "<tr><td>25.29</td><td>4.71</td><td>Male</td><td>No</td><td>Sun</td><td>Dinner</td><td>4</td></tr>\n",
              "<tr><td>8.77</td><td>2.0</td><td>Male</td><td>No</td><td>Sun</td><td>Dinner</td><td>2</td></tr>\n",
              "<tr><td>26.88</td><td>3.12</td><td>Male</td><td>No</td><td>Sun</td><td>Dinner</td><td>4</td></tr>\n",
              "<tr><td>15.04</td><td>1.96</td><td>Male</td><td>No</td><td>Sun</td><td>Dinner</td><td>2</td></tr>\n",
              "<tr><td>14.78</td><td>3.23</td><td>Male</td><td>No</td><td>Sun</td><td>Dinner</td><td>2</td></tr>\n",
              "<tr><td>10.27</td><td>1.71</td><td>Male</td><td>No</td><td>Sun</td><td>Dinner</td><td>2</td></tr>\n",
              "<tr><td>35.26</td><td>5.0</td><td>Female</td><td>No</td><td>Sun</td><td>Dinner</td><td>4</td></tr>\n",
              "<tr><td>15.42</td><td>1.57</td><td>Male</td><td>No</td><td>Sun</td><td>Dinner</td><td>2</td></tr>\n",
              "<tr><td>18.43</td><td>3.0</td><td>Male</td><td>No</td><td>Sun</td><td>Dinner</td><td>4</td></tr>\n",
              "<tr><td>14.83</td><td>3.02</td><td>Female</td><td>No</td><td>Sun</td><td>Dinner</td><td>2</td></tr>\n",
              "<tr><td>21.58</td><td>3.92</td><td>Male</td><td>No</td><td>Sun</td><td>Dinner</td><td>2</td></tr>\n",
              "<tr><td>10.33</td><td>1.67</td><td>Female</td><td>No</td><td>Sun</td><td>Dinner</td><td>3</td></tr>\n",
              "<tr><td>16.29</td><td>3.71</td><td>Male</td><td>No</td><td>Sun</td><td>Dinner</td><td>3</td></tr>\n",
              "<tr><td>16.97</td><td>3.5</td><td>Female</td><td>No</td><td>Sun</td><td>Dinner</td><td>3</td></tr>\n",
              "</table>\n",
              "only showing top 20 rows\n"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(df_pyspark))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4GZKHjY_RlQ",
        "outputId": "24fbc7ba-c7be-4a51-8f02-01efe28995a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pyspark.sql.dataframe.DataFrame'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_pyspark.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gblUuJin_ZMR",
        "outputId": "f047bd95-878d-43d5-a63a-4d7e14251c66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- total_bill: string (nullable = true)\n",
            " |-- tip: string (nullable = true)\n",
            " |-- sex: string (nullable = true)\n",
            " |-- smoker: string (nullable = true)\n",
            " |-- day: string (nullable = true)\n",
            " |-- time: string (nullable = true)\n",
            " |-- size: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[CSV files in Spark](https://spark.apache.org/docs/latest/sql-data-sources-csv.html)"
      ],
      "metadata": {
        "id": "nBHIN_YE-_mI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_pyspark = spark.read.option('header', 'true').csv(path, inferSchema=True)"
      ],
      "metadata": {
        "id": "exDvpWGFypWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the schema\n",
        "\n",
        "df_pyspark.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnfmbQWSy4GL",
        "outputId": "607596e1-ade4-42c7-8902-ab5314b38c1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- total_bill: double (nullable = true)\n",
            " |-- tip: double (nullable = true)\n",
            " |-- sex: string (nullable = true)\n",
            " |-- smoker: string (nullable = true)\n",
            " |-- day: string (nullable = true)\n",
            " |-- time: string (nullable = true)\n",
            " |-- size: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_pyspark.show(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1Sh3PT4z5XB",
        "outputId": "fba21093-7500-4527-e503-aa2c514b4c61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----+------+------+---+------+----+\n",
            "|total_bill| tip|   sex|smoker|day|  time|size|\n",
            "+----------+----+------+------+---+------+----+\n",
            "|     16.99|1.01|Female|    No|Sun|Dinner|   2|\n",
            "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|\n",
            "|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|\n",
            "+----------+----+------+------+---+------+----+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(df_pyspark)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "id": "YMA_bZJUAMvz",
        "outputId": "7d2866d0-8b3c-4a26-8e52-6d29c037aa02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.sql.dataframe.DataFrame"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>pyspark.sql.dataframe.DataFrame</b><br/>def __init__(jdf: JavaObject, sql_ctx: Union[&#x27;SQLContext&#x27;, &#x27;SparkSession&#x27;])</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/content/spark-3.5.1-bin-hadoop3/python/pyspark/sql/dataframe.py</a>A distributed collection of data grouped into named columns.\n",
              "\n",
              ".. versionadded:: 1.3.0\n",
              "\n",
              ".. versionchanged:: 3.4.0\n",
              "    Supports Spark Connect.\n",
              "\n",
              "Examples\n",
              "--------\n",
              "A :class:`DataFrame` is equivalent to a relational table in Spark SQL,\n",
              "and can be created using various functions in :class:`SparkSession`:\n",
              "\n",
              "&gt;&gt;&gt; people = spark.createDataFrame([\n",
              "...     {&quot;deptId&quot;: 1, &quot;age&quot;: 40, &quot;name&quot;: &quot;Hyukjin Kwon&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;salary&quot;: 50},\n",
              "...     {&quot;deptId&quot;: 1, &quot;age&quot;: 50, &quot;name&quot;: &quot;Takuya Ueshin&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;salary&quot;: 100},\n",
              "...     {&quot;deptId&quot;: 2, &quot;age&quot;: 60, &quot;name&quot;: &quot;Xinrong Meng&quot;, &quot;gender&quot;: &quot;F&quot;, &quot;salary&quot;: 150},\n",
              "...     {&quot;deptId&quot;: 3, &quot;age&quot;: 20, &quot;name&quot;: &quot;Haejoon Lee&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;salary&quot;: 200}\n",
              "... ])\n",
              "\n",
              "Once created, it can be manipulated using the various domain-specific-language\n",
              "(DSL) functions defined in: :class:`DataFrame`, :class:`Column`.\n",
              "\n",
              "To select a column from the :class:`DataFrame`, use the apply method:\n",
              "\n",
              "&gt;&gt;&gt; age_col = people.age\n",
              "\n",
              "A more concrete example:\n",
              "\n",
              "&gt;&gt;&gt; # To create DataFrame using SparkSession\n",
              "... department = spark.createDataFrame([\n",
              "...     {&quot;id&quot;: 1, &quot;name&quot;: &quot;PySpark&quot;},\n",
              "...     {&quot;id&quot;: 2, &quot;name&quot;: &quot;ML&quot;},\n",
              "...     {&quot;id&quot;: 3, &quot;name&quot;: &quot;Spark SQL&quot;}\n",
              "... ])\n",
              "\n",
              "&gt;&gt;&gt; people.filter(people.age &gt; 30).join(\n",
              "...     department, people.deptId == department.id).groupBy(\n",
              "...     department.name, &quot;gender&quot;).agg({&quot;salary&quot;: &quot;avg&quot;, &quot;age&quot;: &quot;max&quot;}).show()\n",
              "+-------+------+-----------+--------+\n",
              "|   name|gender|avg(salary)|max(age)|\n",
              "+-------+------+-----------+--------+\n",
              "|     ML|     F|      150.0|      60|\n",
              "|PySpark|     M|       75.0|      50|\n",
              "+-------+------+-----------+--------+\n",
              "\n",
              "Notes\n",
              "-----\n",
              "A DataFrame should only be created as described above. It should not be directly\n",
              "created via using the constructor.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 80);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Selecting Columns and Indexing"
      ],
      "metadata": {
        "id": "iNABoS7UAa9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_pyspark.head(3) # Returns data structure in a list format"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2WvwniaAdF7",
        "outputId": "4c8ca5eb-9fe0-4216-e379-a3499a116c36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(total_bill=16.99, tip=1.01, sex='Female', smoker='No', day='Sun', time='Dinner', size=2),\n",
              " Row(total_bill=10.34, tip=1.66, sex='Male', smoker='No', day='Sun', time='Dinner', size=3),\n",
              " Row(total_bill=21.01, tip=3.5, sex='Male', smoker='No', day='Sun', time='Dinner', size=3)]"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_pyspark.select('sex').show(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyrwJf1cAiWG",
        "outputId": "8fbaaba6-93aa-4075-aff8-33bba967db38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+\n",
            "|   sex|\n",
            "+------+\n",
            "|Female|\n",
            "|  Male|\n",
            "+------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_pyspark.select(['total_bill', 'tip']).show(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sX3diqj1Aw-a",
        "outputId": "8ba3dfd6-0491-4826-e370-970a86c53d1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----+\n",
            "|total_bill| tip|\n",
            "+----------+----+\n",
            "|     16.99|1.01|\n",
            "|     10.34|1.66|\n",
            "|     21.01| 3.5|\n",
            "+----------+----+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_pyspark.dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcfTTpvUBAyo",
        "outputId": "bd82fbc9-6529-40f9-aa3a-d1ed249074b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('total_bill', 'double'),\n",
              " ('tip', 'double'),\n",
              " ('sex', 'string'),\n",
              " ('smoker', 'string'),\n",
              " ('day', 'string'),\n",
              " ('time', 'string'),\n",
              " ('size', 'int')]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Describe Function Similar to Pandas"
      ],
      "metadata": {
        "id": "J_7iozsQBWe4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_pyspark.describe().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYy_p147BRpY",
        "outputId": "236f26ac-a04c-4f2c-adba-e1d6eed4ec5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------+------------------+------+------+----+------+------------------+\n",
            "|summary|        total_bill|               tip|   sex|smoker| day|  time|              size|\n",
            "+-------+------------------+------------------+------+------+----+------+------------------+\n",
            "|  count|               244|               244|   244|   244| 244|   244|               244|\n",
            "|   mean|19.785942622950824|2.9982786885245902|  NULL|  NULL|NULL|  NULL| 2.569672131147541|\n",
            "| stddev| 8.902411954856857|1.3836381890011815|  NULL|  NULL|NULL|  NULL|0.9510998047322347|\n",
            "|    min|              3.07|               1.0|Female|    No| Fri|Dinner|                 1|\n",
            "|    max|             50.81|              10.0|  Male|   Yes|Thur| Lunch|                 6|\n",
            "+-------+------------------+------------------+------+------+----+------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding Columns in Data Frame"
      ],
      "metadata": {
        "id": "wdL1TDf6Bqoo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, when, rand"
      ],
      "metadata": {
        "id": "91BS__VbHZHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets to create a new columns with random numbers between 0 an 1\n",
        "\n",
        "seed = 42\n",
        "\n",
        "df_pyspark = df_pyspark.withColumns(\n",
        "    {\"Visits\": (rand(seed) *10).cast(\"int\") + 1} # Pass column name and expression\n",
        ")\n",
        "df_pyspark.show(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVGwRKrMBo1d",
        "outputId": "181df578-7043-4af2-f7fa-35197a95248e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----+------+------+---+------+----+------+\n",
            "|total_bill| tip|   sex|smoker|day|  time|size|Visits|\n",
            "+----------+----+------+------+---+------+----+------+\n",
            "|     16.99|1.01|Female|    No|Sun|Dinner|   2|     7|\n",
            "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|     6|\n",
            "+----------+----+------+------+---+------+----+------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# New column with more than twice visits\n",
        "df_pyspark = df_pyspark.withColumn(\n",
        "    \"Visited the restaurant more than twice\",\n",
        "    when(col(\"Visits\") > 2, True).otherwise(False)\n",
        ")"
      ],
      "metadata": {
        "id": "4kylY9XYHiVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_pyspark.show(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lk2xlrUsJFNw",
        "outputId": "dab7bc81-7f45-40e9-cd8d-c4fbebf8d498"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----+------+------+---+------+----+------+--------------------------------------+\n",
            "|total_bill| tip|   sex|smoker|day|  time|size|Visits|Visited the restaurant more than twice|\n",
            "+----------+----+------+------+---+------+----+------+--------------------------------------+\n",
            "|     16.99|1.01|Female|    No|Sun|Dinner|   2|     7|                                  true|\n",
            "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|     6|                                  true|\n",
            "+----------+----+------+------+---+------+----+------+--------------------------------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To drop the colums\n",
        "\n",
        "df_pyspark = df_pyspark.drop(\"Visited the restaurant more than twice\")\n",
        "df_pyspark.show(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dK7ZX0E9JyZA",
        "outputId": "26af6aa2-b134-4dd4-bc68-20636a4a675e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----+------+------+---+------+----+------+\n",
            "|total_bill| tip|   sex|smoker|day|  time|size|Visits|\n",
            "+----------+----+------+------+---+------+----+------+\n",
            "|     16.99|1.01|Female|    No|Sun|Dinner|   2|     7|\n",
            "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|     6|\n",
            "+----------+----+------+------+---+------+----+------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename the columns\n",
        "df_pyspark = df_pyspark.withColumnRenamed('sex','genre')\n",
        "df_pyspark.show(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJoTZkK_KNwo",
        "outputId": "9e9ed7a9-8066-45ef-feb7-1d2c840ec663"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----+------+------+---+------+----+------+--------------------------------------+\n",
            "|total_bill| tip| genre|smoker|day|  time|size|Visits|Visited the restaurant more than twice|\n",
            "+----------+----+------+------+---+------+----+------+--------------------------------------+\n",
            "|     16.99|1.01|Female|    No|Sun|Dinner|   2|     7|                                  true|\n",
            "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|     6|                                  true|\n",
            "+----------+----+------+------+---+------+----+------+--------------------------------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handling Missing Values"
      ],
      "metadata": {
        "id": "3ey1qKcxLzrj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's create a Dataset"
      ],
      "metadata": {
        "id": "UYfg4328KR30"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import lit, col\n",
        "from pyspark.sql.types import IntegerType, StringType, StructType, StructField"
      ],
      "metadata": {
        "id": "Gi2HZeIAJ5yY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "schema = StructType([\n",
        "    StructField(\"Type of Phenomenon\", StringType(), True),\n",
        "    StructField(\"Year\", IntegerType(), True),\n",
        "    StructField(\"Country\", StringType(), True),\n",
        "    StructField(\"State\", StringType(), True),\n",
        "    StructField(\"Number of Deaths\", IntegerType(), True)\n",
        "])"
      ],
      "metadata": {
        "id": "s9mJQ-yzMCaR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    (\"Landslide\", 2020, \"USA\", \"California\", 15),\n",
        "    (\"Debrisflow\", 2019, \"Japan\", \"Kyoto\", None),  # NaN value for 'Number of Deaths'\n",
        "    (\"Flood\", 2021, \"India\", None, 120),             # NaN value for 'State'\n",
        "    (\"Landslide\", None, \"Brazil\", \"Rio de Janeiro\", 8),  # NaN value for 'Year'\n",
        "    (\"Flood\", 2022, \"China\", \"Guangdong\", None),       # NaN value for 'Number of Deaths'\n",
        "    (\"Debrisflow\", 2018, \"Italy\", \"Rome\", 5)\n",
        "]"
      ],
      "metadata": {
        "id": "6qZTTMVMMHDO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_natural_disasters = spark.createDataFrame(data, schema=schema)\n",
        "df_natural_disasters.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34M3f5ZAMKHm",
        "outputId": "5c76f36c-a012-4b84-824c-4d74c65fd610"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+----+-------+--------------+----------------+\n",
            "|Type of Phenomenon|Year|Country|         State|Number of Deaths|\n",
            "+------------------+----+-------+--------------+----------------+\n",
            "|         Landslide|2020|    USA|    California|              15|\n",
            "|        Debrisflow|2019|  Japan|         Kyoto|            NULL|\n",
            "|             Flood|2021|  India|          NULL|             120|\n",
            "|         Landslide|NULL| Brazil|Rio de Janeiro|               8|\n",
            "|             Flood|2022|  China|     Guangdong|            NULL|\n",
            "|        Debrisflow|2018|  Italy|          Rome|               5|\n",
            "+------------------+----+-------+--------------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Drop NaN Values\n",
        "print(\"DataFrame by default drop values\")\n",
        "df_natural_disasters.na.drop().show()\n",
        "\n",
        "## If we use the parameters of the na property:\n",
        "print(\"DataFrame with `how` and using `threshold` parameters\")\n",
        "df_natural_disasters.na.drop(how=\"any\", thresh=2).show()\n",
        "\n",
        "print(\"DataFrame with `how` and `subset` parameters\")\n",
        "df_natural_disasters.na.drop(how=\"any\", subset=[\"Number of Deaths\"]).show()"
      ],
      "metadata": {
        "id": "RcLcsbYzMQLy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cf99b22-e39c-4a97-caec-af94155e0e0f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame by default drop values\n",
            "+------------------+----+-------+----------+----------------+\n",
            "|Type of Phenomenon|Year|Country|     State|Number of Deaths|\n",
            "+------------------+----+-------+----------+----------------+\n",
            "|         Landslide|2020|    USA|California|              15|\n",
            "|        Debrisflow|2018|  Italy|      Rome|               5|\n",
            "+------------------+----+-------+----------+----------------+\n",
            "\n",
            "DataFrame with how = `any`\n",
            "+------------------+----+-------+--------------+----------------+\n",
            "|Type of Phenomenon|Year|Country|         State|Number of Deaths|\n",
            "+------------------+----+-------+--------------+----------------+\n",
            "|         Landslide|2020|    USA|    California|              15|\n",
            "|        Debrisflow|2019|  Japan|         Kyoto|            NULL|\n",
            "|             Flood|2021|  India|          NULL|             120|\n",
            "|         Landslide|NULL| Brazil|Rio de Janeiro|               8|\n",
            "|             Flood|2022|  China|     Guangdong|            NULL|\n",
            "|        Debrisflow|2018|  Italy|          Rome|               5|\n",
            "+------------------+----+-------+--------------+----------------+\n",
            "\n",
            "DataFrame with `how` and `subset` parameters\n",
            "+------------------+----+-------+--------------+----------------+\n",
            "|Type of Phenomenon|Year|Country|         State|Number of Deaths|\n",
            "+------------------+----+-------+--------------+----------------+\n",
            "|         Landslide|2020|    USA|    California|              15|\n",
            "|             Flood|2021|  India|          NULL|             120|\n",
            "|         Landslide|NULL| Brazil|Rio de Janeiro|               8|\n",
            "|        Debrisflow|2018|  Italy|          Rome|               5|\n",
            "+------------------+----+-------+--------------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Filling the missing values\n",
        "\n",
        "df_natural_disasters.na.fill(\"MISSING VALUES\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pa2zblLYK29z",
        "outputId": "6a9db2ee-3dea-43a1-a4ed-5db7913c7c40"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+----+-------+--------------+----------------+\n",
            "|Type of Phenomenon|Year|Country|         State|Number of Deaths|\n",
            "+------------------+----+-------+--------------+----------------+\n",
            "|         Landslide|2020|    USA|    California|              15|\n",
            "|        Debrisflow|2019|  Japan|         Kyoto|            NULL|\n",
            "|             Flood|2021|  India|MISSING VALUES|             120|\n",
            "|         Landslide|NULL| Brazil|Rio de Janeiro|               8|\n",
            "|             Flood|2022|  China|     Guangdong|            NULL|\n",
            "|        Debrisflow|2018|  Italy|          Rome|               5|\n",
            "+------------------+----+-------+--------------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To impute the values using advanced techniques\n",
        "\n",
        "from pyspark.ml.feature import Imputer\n",
        "\n",
        "imputer = Imputer(\n",
        "    inputCols=[\"Number of Deaths\"],\n",
        "    outputCols=[\"{}_imputed\".format(c) for c in [\"Number of Deaths\"]]\n",
        ").setStrategy(\"mean\")"
      ],
      "metadata": {
        "id": "2EBT-IJmMA3A"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imputer.fit(df_natural_disasters).transform(df_natural_disasters).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03lQGxL-Q-RY",
        "outputId": "f3f196df-c06e-41ee-dacd-b8d6e943b935"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+----+-------+--------------+----------------+------------------------+\n",
            "|Type of Phenomenon|Year|Country|         State|Number of Deaths|Number of Deaths_imputed|\n",
            "+------------------+----+-------+--------------+----------------+------------------------+\n",
            "|         Landslide|2020|    USA|    California|              15|                      15|\n",
            "|        Debrisflow|2019|  Japan|         Kyoto|            NULL|                      37|\n",
            "|             Flood|2021|  India|          NULL|             120|                     120|\n",
            "|         Landslide|NULL| Brazil|Rio de Janeiro|               8|                       8|\n",
            "|             Flood|2022|  China|     Guangdong|            NULL|                      37|\n",
            "|        Debrisflow|2018|  Italy|          Rome|               5|                       5|\n",
            "+------------------+----+-------+--------------+----------------+------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Filter operation"
      ],
      "metadata": {
        "id": "XfuqqEToSdeb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using SQL syntaxis\n",
        "df_natural_disasters.filter(\"`Number of Deaths`<10\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5MEwtOCRCe0",
        "outputId": "c9efcef6-4200-4f57-d32c-21c4196f8255"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+----+-------+--------------+----------------+\n",
            "|Type of Phenomenon|Year|Country|         State|Number of Deaths|\n",
            "+------------------+----+-------+--------------+----------------+\n",
            "|         Landslide|NULL| Brazil|Rio de Janeiro|               8|\n",
            "|        Debrisflow|2018|  Italy|          Rome|               5|\n",
            "+------------------+----+-------+--------------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To be more specific\n",
        "df_natural_disasters.filter(\"`Number of Deaths`<10\").select(['Country','Type of Phenomenon'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "ryZmenbUP2C1",
        "outputId": "c10a2d89-9251-4d85-b07c-1dd77a387715"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "+-------+------------------+\n",
              "|Country|Type of Phenomenon|\n",
              "+-------+------------------+\n",
              "| Brazil|         Landslide|\n",
              "|  Italy|        Debrisflow|\n",
              "+-------+------------------+"
            ],
            "text/html": [
              "<table border='1'>\n",
              "<tr><th>Country</th><th>Type of Phenomenon</th></tr>\n",
              "<tr><td>Brazil</td><td>Landslide</td></tr>\n",
              "<tr><td>Italy</td><td>Debrisflow</td></tr>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using PySpark expressions\n",
        "df_natural_disasters.filter(df_natural_disasters['Number of Deaths']<10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "d7KwEdrfQU9j",
        "outputId": "9a0f0fd5-a3fe-420d-933e-8a17b9e09b47"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "+------------------+----+-------+--------------+----------------+\n",
              "|Type of Phenomenon|Year|Country|         State|Number of Deaths|\n",
              "+------------------+----+-------+--------------+----------------+\n",
              "|         Landslide|NULL| Brazil|Rio de Janeiro|               8|\n",
              "|        Debrisflow|2018|  Italy|          Rome|               5|\n",
              "+------------------+----+-------+--------------+----------------+"
            ],
            "text/html": [
              "<table border='1'>\n",
              "<tr><th>Type of Phenomenon</th><th>Year</th><th>Country</th><th>State</th><th>Number of Deaths</th></tr>\n",
              "<tr><td>Landslide</td><td>NULL</td><td>Brazil</td><td>Rio de Janeiro</td><td>8</td></tr>\n",
              "<tr><td>Debrisflow</td><td>2018</td><td>Italy</td><td>Rome</td><td>5</td></tr>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using different conditions\n",
        "\n",
        "df_natural_disasters.filter((df_natural_disasters['Number of Deaths']>10) &\n",
        "                            (df_natural_disasters['Type of Phenomenon']== 'Landslide'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "id": "lBGORVhpRQR8",
        "outputId": "36d14587-fb00-417a-aaa3-68993831eec1"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "+------------------+----+-------+----------+----------------+\n",
              "|Type of Phenomenon|Year|Country|     State|Number of Deaths|\n",
              "+------------------+----+-------+----------+----------------+\n",
              "|         Landslide|2020|    USA|California|              15|\n",
              "+------------------+----+-------+----------+----------------+"
            ],
            "text/html": [
              "<table border='1'>\n",
              "<tr><th>Type of Phenomenon</th><th>Year</th><th>Country</th><th>State</th><th>Number of Deaths</th></tr>\n",
              "<tr><td>Landslide</td><td>2020</td><td>USA</td><td>California</td><td>15</td></tr>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GroupBy and Aggregate Functions"
      ],
      "metadata": {
        "id": "TvQKEbFBT37p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create more rows."
      ],
      "metadata": {
        "id": "bn2-ypW2h6Rh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "new_data = [\n",
        "    Row(\"Earthquake\", 2023, \"Japan\", \"Kanto\", 150),\n",
        "    Row(\"Flood\", 2022, \"India\", \"Kerala\", 40),\n",
        "    Row(\"Hurricane\", 2021, \"USA\", \"Florida\", 85),\n",
        "    Row(\"Tornado\", 2020, \"USA\", \"Oklahoma\", 30),\n",
        "    Row(\"Volcano Eruption\", 2019, \"Indonesia\", \"Java\", 70),\n",
        "    Row(\"Earthquake\", 2018, \"Mexico\", \"Oaxaca\", 90),\n",
        "    Row(\"Flood\", 2023, \"Bangladesh\", \"Dhaka\", 60),\n",
        "    Row(\"Tsunami\", 2022, \"Chile\", \"Valparaíso\", 200),\n",
        "    Row(\"Drought\", 2021, \"Australia\", \"New South Wales\", 5),\n",
        "    Row(\"Heatwave\", 2020, \"France\", \"Paris\", 110),\n",
        "    Row(\"Earthquake\", 2019, \"Nepal\", \"Kathmandu\", 50),\n",
        "    Row(\"Flood\", 2018, \"China\", \"Guangxi\", 45),\n",
        "    Row(\"Hurricane\", 2023, \"Cuba\", \"Havana\", 95),\n",
        "    Row(\"Wildfire\", 2022, \"Brazil\", \"Amazonas\", 25),\n",
        "    Row(\"Tornado\", 2021, \"Canada\", \"Ontario\", 15),\n",
        "    Row(\"Earthquake\", 2020, \"Turkey\", \"Izmir\", 65),\n",
        "    Row(\"Flood\", 2019, \"Italy\", \"Veneto\", 20),\n",
        "    Row(\"Hurricane\", 2018, \"Philippines\", \"Luzon\", 180),\n",
        "    Row(\"Volcano Eruption\", 2023, \"Iceland\", \"Reykjavik\", 10),\n",
        "    Row(\"Earthquake\", 2022, \"Peru\", \"Lima\", 45),\n",
        "    Row(\"Flood\", 2021, \"Pakistan\", \"Sindh\", 80),\n",
        "    Row(\"Hurricane\", 2020, \"Mexico\", \"Yucatan\", 50),\n",
        "    Row(\"Tornado\", 2019, \"Argentina\", \"Santa Fe\", 35),\n",
        "    Row(\"Volcano Eruption\", 2018, \"Italy\", \"Sicily\", 55),\n",
        "    Row(\"Earthquake\", 2023, \"Turkey\", \"Ankara\", 130),\n",
        "    Row(\"Flood\", 2022, \"Nigeria\", \"Lagos\", 60),\n",
        "    Row(\"Hurricane\", 2021, \"Dominican Republic\", \"Santo Domingo\", 75),\n",
        "    Row(\"Wildfire\", 2020, \"Australia\", \"Victoria\", 40),\n",
        "    Row(\"Tornado\", 2019, \"USA\", \"Kansas\", 20),\n",
        "    Row(\"Earthquake\", 2018, \"Greece\", \"Athens\", 85),\n",
        "    Row(\"Flood\", 2023, \"Brazil\", \"Rio de Janeiro\", 70),\n",
        "    Row(\"Earthquake\", 2022, \"Chile\", \"Santiago\", 120),\n",
        "    Row(\"Hurricane\", 2021, \"Cuba\", \"Havana\", 95),\n",
        "    Row(\"Tsunami\", 2020, \"Peru\", \"Lima\", 200),\n",
        "    Row(\"Volcano Eruption\", 2019, \"Ecuador\", \"Quito\", 80),\n",
        "    Row(\"Flood\", 2018, \"Argentina\", \"Buenos Aires\", 30),\n",
        "    Row(\"Earthquake\", 2023, \"Mexico\", \"Mexico City\", 160),\n",
        "    Row(\"Drought\", 2022, \"Venezuela\", \"Caracas\", 10),\n",
        "    Row(\"Hurricane\", 2021, \"Dominican Republic\", \"Santo Domingo\", 75),\n",
        "    Row(\"Wildfire\", 2020, \"Bolivia\", \"Santa Cruz\", 45),\n",
        "    Row(\"Flood\", 2019, \"Colombia\", \"Bogotá\", 55),\n",
        "    Row(\"Tornado\", 2018, \"Paraguay\", \"Asuncion\", 15),\n",
        "    Row(\"Volcano Eruption\", 2023, \"Guatemala\", \"Antigua\", 100),\n",
        "    Row(\"Flood\", 2022, \"Uruguay\", \"Montevideo\", 20),\n",
        "    Row(\"Hurricane\", 2021, \"Honduras\", \"Tegucigalpa\", 65),\n",
        "    Row(\"Tsunami\", 2020, \"El Salvador\", \"San Salvador\", 150),\n",
        "    Row(\"Wildfire\", 2019, \"Panama\", \"Panama City\", 35),\n",
        "    Row(\"Earthquake\", 2018, \"Nicaragua\", \"Managua\", 60),\n",
        "    Row(\"Flood\", 2023, \"Costa Rica\", \"San Jose\", 25),\n",
        "    Row(\"Volcano Eruption\", 2022, \"Nicaragua\", \"Leon\", 90),\n",
        "    Row(\"Hurricane\", 2021, \"Belize\", \"Belize City\", 50),\n",
        "    Row(\"Earthquake\", 2020, \"Haiti\", \"Port-au-Prince\", 180),\n",
        "    Row(\"Flood\", 2019, \"Guatemala\", \"Guatemala City\", 40),\n",
        "    Row(\"Wildfire\", 2018, \"Chile\", \"Valparaíso\", 30),\n",
        "    Row(\"Tornado\", 2023, \"Argentina\", \"Cordoba\", 25),\n",
        "    Row(\"Hurricane\", 2022, \"Mexico\", \"Cancun\", 80),\n",
        "    Row(\"Flood\", 2021, \"Colombia\", \"Medellin\", 60),\n",
        "    Row(\"Volcano Eruption\", 2020, \"Ecuador\", \"Guayaquil\", 70),\n",
        "    Row(\"Earthquake\", 2019, \"Peru\", \"Arequipa\", 50),\n",
        "    Row(\"Wildfire\", 2018, \"Brazil\", \"Sao Paulo\", 20),\n",
        "    Row(\"Tsunami\", 2023, \"Chile\", \"La Serena\", 110),\n",
        "    Row(\"Drought\", 2022, \"Mexico\", \"Chihuahua\", 15),\n",
        "    Row(\"Flood\", 2021, \"Venezuela\", \"Maracaibo\", 45),\n",
        "    Row(\"Hurricane\", 2020, \"Nicaragua\", \"Bluefields\", 85),\n",
        "    Row(\"Tornado\", 2019, \"Brazil\", \"Porto Alegre\", 35)\n",
        "]"
      ],
      "metadata": {
        "id": "p92upO4Xh980"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cretae a DataFrame with the new rows\n",
        "\n",
        "new_df = spark.createDataFrame(new_data, schema=[\"Type of Phenomenon\", \"Year\", \"Country\", \"State\", \"Number of Deaths\"])\n",
        "\n",
        "df_natural_disasters = df_natural_disasters.union(new_df)"
      ],
      "metadata": {
        "id": "oh3IvkKPjYM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pyspark(df):\n",
        "    num_rows = df.count()\n",
        "    num_cols = len(df.columns)\n",
        "    print(f\"Number of rows: {num_rows}\")\n",
        "    print(f\"Number of columns: {num_cols}\")\n",
        "    return"
      ],
      "metadata": {
        "id": "6Il9xSq_jYQQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pyspark(df_natural_disasters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3W0Q1jKjYTf",
        "outputId": "d17cd33e-fb1b-4024-c84d-bf8238d7bd16"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows: 71\n",
            "Number of columns: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_natural_disasters.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FR0EAHs9SOTi",
        "outputId": "caed5c08-6050-42fe-9df4-a100b69e682a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Type of Phenomenon', 'Year', 'Country', 'State', 'Number of Deaths']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_natural_disasters.dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsrDJrxzbD5d",
        "outputId": "a70c48cb-2381-469c-cc90-5d795406bbd5"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Type of Phenomenon', 'string'),\n",
              " ('Year', 'int'),\n",
              " ('Country', 'string'),\n",
              " ('State', 'string'),\n",
              " ('Number of Deaths', 'int')]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GroupBy\n",
        "\n",
        "df_natural_disasters.groupBy('Country').sum('Number of Deaths').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWAx-8t_bJEb",
        "outputId": "dc0ae4c2-d7c0-4c0a-b64c-6261bbeb369a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+---------------------+\n",
            "|    Country|sum(Number of Deaths)|\n",
            "+-----------+---------------------+\n",
            "|      India|                  160|\n",
            "|        USA|                  150|\n",
            "|      Japan|                  150|\n",
            "|      China|                   45|\n",
            "|      Italy|                   80|\n",
            "|     Brazil|                  158|\n",
            "|Philippines|                  180|\n",
            "|     Turkey|                  195|\n",
            "|     France|                  110|\n",
            "|     Greece|                   85|\n",
            "|  Argentina|                   90|\n",
            "|       Peru|                  295|\n",
            "|      Chile|                  460|\n",
            "|    Nigeria|                   60|\n",
            "|       Cuba|                  190|\n",
            "| Bangladesh|                   60|\n",
            "|    Iceland|                   10|\n",
            "|     Mexico|                  395|\n",
            "|  Indonesia|                   70|\n",
            "|     Canada|                   15|\n",
            "+-----------+---------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_natural_disasters.groupBy('Country').count().orderBy('count', ascending=False).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAhTJJM9ldYo",
        "outputId": "99a4d1ff-5973-4e3a-8d33-5a53b008d37a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+-----+\n",
            "|           Country|count|\n",
            "+------------------+-----+\n",
            "|            Brazil|    5|\n",
            "|            Mexico|    5|\n",
            "|               USA|    4|\n",
            "|             Chile|    4|\n",
            "|             Italy|    3|\n",
            "|         Argentina|    3|\n",
            "|              Peru|    3|\n",
            "|         Nicaragua|    3|\n",
            "|             India|    2|\n",
            "|             Japan|    2|\n",
            "|             China|    2|\n",
            "|            Turkey|    2|\n",
            "|              Cuba|    2|\n",
            "|Dominican Republic|    2|\n",
            "|         Australia|    2|\n",
            "|           Ecuador|    2|\n",
            "|         Venezuela|    2|\n",
            "|         Guatemala|    2|\n",
            "|          Colombia|    2|\n",
            "|       Philippines|    1|\n",
            "+------------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_natural_disasters.agg({'Number of Deaths': 'sum'}).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIO2IEUomdIi",
        "outputId": "6d45039b-332d-4177-c964-800bce2501d0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------+\n",
            "|sum(Number of Deaths)|\n",
            "+---------------------+\n",
            "|                 4563|\n",
            "+---------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLlib"
      ],
      "metadata": {
        "id": "Pl3lOqajqhcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MLlib (DataFrame-based)](https://spark.apache.org/docs/latest/api/python/reference/pyspark.ml.html)\n",
        "\n",
        "[Machine Learning Library (MLlib) Guide](https://spark.apache.org/docs/latest/ml-guide)"
      ],
      "metadata": {
        "id": "dLGs0Ui3qsIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_natural_disasters.write.parquet(\"path/to/export/df_natural_disasters.parquet\")\n"
      ],
      "metadata": {
        "id": "7478nj7DqrWE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}