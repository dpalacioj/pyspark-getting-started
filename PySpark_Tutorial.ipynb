{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["iVqtVB1TKHz9"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/dpalacioj/pyspark-getting-started/blob/main/PySpark_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","source":["# Upload Drive and Spark System\n","\n"],"metadata":{"id":"MAnPeRldHFob"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mJHRs1oQw1dy","outputId":"dd5399c2-bed3-40a7-ffe5-6b902fad3b2f","executionInfo":{"status":"ok","timestamp":1725202425712,"user_tz":300,"elapsed":18944,"user":{"displayName":"DAVID PALACIO J.","userId":"15633330119049240004"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XRIi2yKwt_Mt","outputId":"4ea56c09-3e67-4994-b09d-4d7c65fc5f9d","executionInfo":{"status":"ok","timestamp":1725202476888,"user_tz":300,"elapsed":51178,"user":{"displayName":"DAVID PALACIO J.","userId":"15633330119049240004"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 399 ms, sys: 43.9 ms, total: 443 ms\n","Wall time: 51.2 s\n"]}],"source":["%%time\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","!wget -q http://archive.apache.org/dist/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz\n","!tar xf spark-3.5.1-bin-hadoop3.tgz\n","!pip install -q findspark"]},{"cell_type":"markdown","source":["## Set Enviroment Variables\n","\n"],"metadata":{"id":"cCuSwOGWuU2u"}},{"cell_type":"code","source":["import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.1-bin-hadoop3\""],"metadata":{"id":"ZB-cQZYkuFW8","executionInfo":{"status":"ok","timestamp":1725202476888,"user_tz":300,"elapsed":6,"user":{"displayName":"DAVID PALACIO J.","userId":"15633330119049240004"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wVdJKPnEudaF","outputId":"b6b401f8-bd2b-49bf-8587-477e7644a5b3","executionInfo":{"status":"ok","timestamp":1725202491015,"user_tz":300,"elapsed":7,"user":{"displayName":"DAVID PALACIO J.","userId":"15633330119049240004"}}},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["drive  sample_data  spark-3.5.1-bin-hadoop3  spark-3.5.1-bin-hadoop3.tgz\n"]}]},{"cell_type":"code","source":["import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.master(\"local[*]\").getOrCreate() # Always to create a Session\n","spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # Property used to format output tables better\n","spark"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":219},"id":"uSgQmAEyueEg","outputId":"e9d02909-4f43-47f4-cab7-b9f2acab2737","executionInfo":{"status":"ok","timestamp":1725202491015,"user_tz":300,"elapsed":14132,"user":{"displayName":"DAVID PALACIO J.","userId":"15633330119049240004"}}},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<pyspark.sql.session.SparkSession at 0x7da18072fe50>"],"text/html":["\n","            <div>\n","                <p><b>SparkSession - in-memory</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://9b2c5e9530df:4040\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.5.1</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[*]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>pyspark-shell</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["# Uploading Data"],"metadata":{"id":"iVqtVB1TKHz9"}},{"cell_type":"code","source":["path = r\"/content/drive/MyDrive/big-data/pyspark-getting-started/data/tips.csv\""],"metadata":{"id":"5kyij8gd-aXC","executionInfo":{"status":"ok","timestamp":1725202978156,"user_tz":300,"elapsed":251,"user":{"displayName":"DAVID PALACIO J.","userId":"15633330119049240004"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["df_pyspark=spark.read.csv(path)"],"metadata":{"id":"0lcJv_eOuwyt","executionInfo":{"status":"ok","timestamp":1725202988947,"user_tz":300,"elapsed":9877,"user":{"displayName":"DAVID PALACIO J.","userId":"15633330119049240004"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["df_pyspark"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":505},"id":"-io-YkS4uy_z","outputId":"b63be29e-bdda-457e-8172-bc3ca2cadf33"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["+----------+----+------+------+---+------+----+\n","|       _c0| _c1|   _c2|   _c3|_c4|   _c5| _c6|\n","+----------+----+------+------+---+------+----+\n","|total_bill| tip|   sex|smoker|day|  time|size|\n","|     16.99|1.01|Female|    No|Sun|Dinner|   2|\n","|     10.34|1.66|  Male|    No|Sun|Dinner|   3|\n","|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|\n","|     23.68|3.31|  Male|    No|Sun|Dinner|   2|\n","|     24.59|3.61|Female|    No|Sun|Dinner|   4|\n","|     25.29|4.71|  Male|    No|Sun|Dinner|   4|\n","|      8.77| 2.0|  Male|    No|Sun|Dinner|   2|\n","|     26.88|3.12|  Male|    No|Sun|Dinner|   4|\n","|     15.04|1.96|  Male|    No|Sun|Dinner|   2|\n","|     14.78|3.23|  Male|    No|Sun|Dinner|   2|\n","|     10.27|1.71|  Male|    No|Sun|Dinner|   2|\n","|     35.26| 5.0|Female|    No|Sun|Dinner|   4|\n","|     15.42|1.57|  Male|    No|Sun|Dinner|   2|\n","|     18.43| 3.0|  Male|    No|Sun|Dinner|   4|\n","|     14.83|3.02|Female|    No|Sun|Dinner|   2|\n","|     21.58|3.92|  Male|    No|Sun|Dinner|   2|\n","|     10.33|1.67|Female|    No|Sun|Dinner|   3|\n","|     16.29|3.71|  Male|    No|Sun|Dinner|   3|\n","|     16.97| 3.5|Female|    No|Sun|Dinner|   3|\n","+----------+----+------+------+---+------+----+\n","only showing top 20 rows"],"text/html":["<table border='1'>\n","<tr><th>_c0</th><th>_c1</th><th>_c2</th><th>_c3</th><th>_c4</th><th>_c5</th><th>_c6</th></tr>\n","<tr><td>total_bill</td><td>tip</td><td>sex</td><td>smoker</td><td>day</td><td>time</td><td>size</td></tr>\n","<tr><td>16.99</td><td>1.01</td><td>Female</td><td>No</td><td>Sun</td><td>Dinner</td><td>2</td></tr>\n","<tr><td>10.34</td><td>1.66</td><td>Male</td><td>No</td><td>Sun</td><td>Dinner</td><td>3</td></tr>\n","<tr><td>21.01</td><td>3.5</td><td>Male</td><td>No</td><td>Sun</td><td>Dinner</td><td>3</td></tr>\n","<tr><td>23.68</td><td>3.31</td><td>Male</td><td>No</td><td>Sun</td><td>Dinner</td><td>2</td></tr>\n","<tr><td>24.59</td><td>3.61</td><td>Female</td><td>No</td><td>Sun</td><td>Dinner</td><td>4</td></tr>\n","<tr><td>25.29</td><td>4.71</td><td>Male</td><td>No</td><td>Sun</td><td>Dinner</td><td>4</td></tr>\n","<tr><td>8.77</td><td>2.0</td><td>Male</td><td>No</td><td>Sun</td><td>Dinner</td><td>2</td></tr>\n","<tr><td>26.88</td><td>3.12</td><td>Male</td><td>No</td><td>Sun</td><td>Dinner</td><td>4</td></tr>\n","<tr><td>15.04</td><td>1.96</td><td>Male</td><td>No</td><td>Sun</td><td>Dinner</td><td>2</td></tr>\n","<tr><td>14.78</td><td>3.23</td><td>Male</td><td>No</td><td>Sun</td><td>Dinner</td><td>2</td></tr>\n","<tr><td>10.27</td><td>1.71</td><td>Male</td><td>No</td><td>Sun</td><td>Dinner</td><td>2</td></tr>\n","<tr><td>35.26</td><td>5.0</td><td>Female</td><td>No</td><td>Sun</td><td>Dinner</td><td>4</td></tr>\n","<tr><td>15.42</td><td>1.57</td><td>Male</td><td>No</td><td>Sun</td><td>Dinner</td><td>2</td></tr>\n","<tr><td>18.43</td><td>3.0</td><td>Male</td><td>No</td><td>Sun</td><td>Dinner</td><td>4</td></tr>\n","<tr><td>14.83</td><td>3.02</td><td>Female</td><td>No</td><td>Sun</td><td>Dinner</td><td>2</td></tr>\n","<tr><td>21.58</td><td>3.92</td><td>Male</td><td>No</td><td>Sun</td><td>Dinner</td><td>2</td></tr>\n","<tr><td>10.33</td><td>1.67</td><td>Female</td><td>No</td><td>Sun</td><td>Dinner</td><td>3</td></tr>\n","<tr><td>16.29</td><td>3.71</td><td>Male</td><td>No</td><td>Sun</td><td>Dinner</td><td>3</td></tr>\n","<tr><td>16.97</td><td>3.5</td><td>Female</td><td>No</td><td>Sun</td><td>Dinner</td><td>3</td></tr>\n","</table>\n","only showing top 20 rows\n"]},"metadata":{},"execution_count":66}]},{"cell_type":"code","source":["print(type(df_pyspark))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z4GZKHjY_RlQ","outputId":"24fbc7ba-c7be-4a51-8f02-01efe28995a0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pyspark.sql.dataframe.DataFrame'>\n"]}]},{"cell_type":"code","source":["df_pyspark.printSchema()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gblUuJin_ZMR","outputId":"f047bd95-878d-43d5-a63a-4d7e14251c66"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["root\n"," |-- total_bill: string (nullable = true)\n"," |-- tip: string (nullable = true)\n"," |-- sex: string (nullable = true)\n"," |-- smoker: string (nullable = true)\n"," |-- day: string (nullable = true)\n"," |-- time: string (nullable = true)\n"," |-- size: string (nullable = true)\n","\n"]}]},{"cell_type":"markdown","source":["[CSV files in Spark](https://spark.apache.org/docs/latest/sql-data-sources-csv.html)"],"metadata":{"id":"nBHIN_YE-_mI"}},{"cell_type":"code","source":["df_pyspark = spark.read.option('header', 'true').csv(path, inferSchema=True)"],"metadata":{"id":"exDvpWGFypWG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check the schema\n","\n","df_pyspark.printSchema()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MnfmbQWSy4GL","outputId":"607596e1-ade4-42c7-8902-ab5314b38c1a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["root\n"," |-- total_bill: double (nullable = true)\n"," |-- tip: double (nullable = true)\n"," |-- sex: string (nullable = true)\n"," |-- smoker: string (nullable = true)\n"," |-- day: string (nullable = true)\n"," |-- time: string (nullable = true)\n"," |-- size: integer (nullable = true)\n","\n"]}]},{"cell_type":"code","source":["df_pyspark.show(3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I1Sh3PT4z5XB","outputId":"fba21093-7500-4527-e503-aa2c514b4c61"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+----------+----+------+------+---+------+----+\n","|total_bill| tip|   sex|smoker|day|  time|size|\n","+----------+----+------+------+---+------+----+\n","|     16.99|1.01|Female|    No|Sun|Dinner|   2|\n","|     10.34|1.66|  Male|    No|Sun|Dinner|   3|\n","|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|\n","+----------+----+------+------+---+------+----+\n","only showing top 3 rows\n","\n"]}]},{"cell_type":"code","source":["type(df_pyspark)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":186},"id":"YMA_bZJUAMvz","outputId":"7d2866d0-8b3c-4a26-8e52-6d29c037aa02"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["pyspark.sql.dataframe.DataFrame"],"text/html":["<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n","      pre.function-repr-contents {\n","        overflow-x: auto;\n","        padding: 8px 12px;\n","        max-height: 500px;\n","      }\n","\n","      pre.function-repr-contents.function-repr-contents-collapsed {\n","        cursor: pointer;\n","        max-height: 100px;\n","      }\n","    </style>\n","    <pre style=\"white-space: initial; background:\n","         var(--colab-secondary-surface-color); padding: 8px 12px;\n","         border-bottom: 1px solid var(--colab-border-color);\"><b>pyspark.sql.dataframe.DataFrame</b><br/>def __init__(jdf: JavaObject, sql_ctx: Union[&#x27;SQLContext&#x27;, &#x27;SparkSession&#x27;])</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/content/spark-3.5.1-bin-hadoop3/python/pyspark/sql/dataframe.py</a>A distributed collection of data grouped into named columns.\n","\n",".. versionadded:: 1.3.0\n","\n",".. versionchanged:: 3.4.0\n","    Supports Spark Connect.\n","\n","Examples\n","--------\n","A :class:`DataFrame` is equivalent to a relational table in Spark SQL,\n","and can be created using various functions in :class:`SparkSession`:\n","\n","&gt;&gt;&gt; people = spark.createDataFrame([\n","...     {&quot;deptId&quot;: 1, &quot;age&quot;: 40, &quot;name&quot;: &quot;Hyukjin Kwon&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;salary&quot;: 50},\n","...     {&quot;deptId&quot;: 1, &quot;age&quot;: 50, &quot;name&quot;: &quot;Takuya Ueshin&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;salary&quot;: 100},\n","...     {&quot;deptId&quot;: 2, &quot;age&quot;: 60, &quot;name&quot;: &quot;Xinrong Meng&quot;, &quot;gender&quot;: &quot;F&quot;, &quot;salary&quot;: 150},\n","...     {&quot;deptId&quot;: 3, &quot;age&quot;: 20, &quot;name&quot;: &quot;Haejoon Lee&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;salary&quot;: 200}\n","... ])\n","\n","Once created, it can be manipulated using the various domain-specific-language\n","(DSL) functions defined in: :class:`DataFrame`, :class:`Column`.\n","\n","To select a column from the :class:`DataFrame`, use the apply method:\n","\n","&gt;&gt;&gt; age_col = people.age\n","\n","A more concrete example:\n","\n","&gt;&gt;&gt; # To create DataFrame using SparkSession\n","... department = spark.createDataFrame([\n","...     {&quot;id&quot;: 1, &quot;name&quot;: &quot;PySpark&quot;},\n","...     {&quot;id&quot;: 2, &quot;name&quot;: &quot;ML&quot;},\n","...     {&quot;id&quot;: 3, &quot;name&quot;: &quot;Spark SQL&quot;}\n","... ])\n","\n","&gt;&gt;&gt; people.filter(people.age &gt; 30).join(\n","...     department, people.deptId == department.id).groupBy(\n","...     department.name, &quot;gender&quot;).agg({&quot;salary&quot;: &quot;avg&quot;, &quot;age&quot;: &quot;max&quot;}).show()\n","+-------+------+-----------+--------+\n","|   name|gender|avg(salary)|max(age)|\n","+-------+------+-----------+--------+\n","|     ML|     F|      150.0|      60|\n","|PySpark|     M|       75.0|      50|\n","+-------+------+-----------+--------+\n","\n","Notes\n","-----\n","A DataFrame should only be created as described above. It should not be directly\n","created via using the constructor.</pre>\n","      <script>\n","      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n","        for (const element of document.querySelectorAll('.filepath')) {\n","          element.style.display = 'block'\n","          element.onclick = (event) => {\n","            event.preventDefault();\n","            event.stopPropagation();\n","            google.colab.files.view(element.textContent, 80);\n","          };\n","        }\n","      }\n","      for (const element of document.querySelectorAll('.function-repr-contents')) {\n","        element.onclick = (event) => {\n","          event.preventDefault();\n","          event.stopPropagation();\n","          element.classList.toggle('function-repr-contents-collapsed');\n","        };\n","      }\n","      </script>\n","      </div>"]},"metadata":{},"execution_count":43}]},{"cell_type":"markdown","source":["# Selecting Columns and Indexing"],"metadata":{"id":"iNABoS7UAa9H"}},{"cell_type":"code","source":["df_pyspark.head(3) # Returns data structure in a list format"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P2WvwniaAdF7","outputId":"4c8ca5eb-9fe0-4216-e379-a3499a116c36"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Row(total_bill=16.99, tip=1.01, sex='Female', smoker='No', day='Sun', time='Dinner', size=2),\n"," Row(total_bill=10.34, tip=1.66, sex='Male', smoker='No', day='Sun', time='Dinner', size=3),\n"," Row(total_bill=21.01, tip=3.5, sex='Male', smoker='No', day='Sun', time='Dinner', size=3)]"]},"metadata":{},"execution_count":70}]},{"cell_type":"code","source":["df_pyspark.select('sex').show(2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GyrwJf1cAiWG","outputId":"8fbaaba6-93aa-4075-aff8-33bba967db38"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+------+\n","|   sex|\n","+------+\n","|Female|\n","|  Male|\n","+------+\n","only showing top 2 rows\n","\n"]}]},{"cell_type":"code","source":["df_pyspark.select(['total_bill', 'tip']).show(3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sX3diqj1Aw-a","outputId":"8ba3dfd6-0491-4826-e370-970a86c53d1f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+----------+----+\n","|total_bill| tip|\n","+----------+----+\n","|     16.99|1.01|\n","|     10.34|1.66|\n","|     21.01| 3.5|\n","+----------+----+\n","only showing top 3 rows\n","\n"]}]},{"cell_type":"code","source":["df_pyspark.dtypes"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hcfTTpvUBAyo","outputId":"bd82fbc9-6529-40f9-aa3a-d1ed249074b3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('total_bill', 'double'),\n"," ('tip', 'double'),\n"," ('sex', 'string'),\n"," ('smoker', 'string'),\n"," ('day', 'string'),\n"," ('time', 'string'),\n"," ('size', 'int')]"]},"metadata":{},"execution_count":52}]},{"cell_type":"markdown","source":["# Describe Function Similar to Pandas"],"metadata":{"id":"J_7iozsQBWe4"}},{"cell_type":"code","source":["df_pyspark.describe().show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YYy_p147BRpY","outputId":"236f26ac-a04c-4f2c-adba-e1d6eed4ec5e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-------+------------------+------------------+------+------+----+------+------------------+\n","|summary|        total_bill|               tip|   sex|smoker| day|  time|              size|\n","+-------+------------------+------------------+------+------+----+------+------------------+\n","|  count|               244|               244|   244|   244| 244|   244|               244|\n","|   mean|19.785942622950824|2.9982786885245902|  NULL|  NULL|NULL|  NULL| 2.569672131147541|\n","| stddev| 8.902411954856857|1.3836381890011815|  NULL|  NULL|NULL|  NULL|0.9510998047322347|\n","|    min|              3.07|               1.0|Female|    No| Fri|Dinner|                 1|\n","|    max|             50.81|              10.0|  Male|   Yes|Thur| Lunch|                 6|\n","+-------+------------------+------------------+------+------+----+------+------------------+\n","\n"]}]},{"cell_type":"markdown","source":["# Adding Columns in Data Frame"],"metadata":{"id":"wdL1TDf6Bqoo"}},{"cell_type":"code","source":["from pyspark.sql.functions import col, when, rand"],"metadata":{"id":"91BS__VbHZHy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Lets to create a new columns with random numbers between 0 an 1\n","\n","seed = 42\n","\n","df_pyspark = df_pyspark.withColumns(\n","    {\"Visits\": (rand(seed) *10).cast(\"int\") + 1} # Pass column name and expression\n",")\n","df_pyspark.show(2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VVGwRKrMBo1d","outputId":"181df578-7043-4af2-f7fa-35197a95248e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+----------+----+------+------+---+------+----+------+\n","|total_bill| tip|   sex|smoker|day|  time|size|Visits|\n","+----------+----+------+------+---+------+----+------+\n","|     16.99|1.01|Female|    No|Sun|Dinner|   2|     7|\n","|     10.34|1.66|  Male|    No|Sun|Dinner|   3|     6|\n","+----------+----+------+------+---+------+----+------+\n","only showing top 2 rows\n","\n"]}]},{"cell_type":"code","source":["# New column with more than twice visits\n","df_pyspark = df_pyspark.withColumn(\n","    \"Visited the restaurant more than twice\",\n","    when(col(\"Visits\") > 2, True).otherwise(False)\n",")"],"metadata":{"id":"4kylY9XYHiVz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_pyspark.show(2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lk2xlrUsJFNw","outputId":"dab7bc81-7f45-40e9-cd8d-c4fbebf8d498"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+----------+----+------+------+---+------+----+------+--------------------------------------+\n","|total_bill| tip|   sex|smoker|day|  time|size|Visits|Visited the restaurant more than twice|\n","+----------+----+------+------+---+------+----+------+--------------------------------------+\n","|     16.99|1.01|Female|    No|Sun|Dinner|   2|     7|                                  true|\n","|     10.34|1.66|  Male|    No|Sun|Dinner|   3|     6|                                  true|\n","+----------+----+------+------+---+------+----+------+--------------------------------------+\n","only showing top 2 rows\n","\n"]}]},{"cell_type":"code","source":["# To drop the colums\n","\n","df_pyspark = df_pyspark.drop(\"Visited the restaurant more than twice\")\n","df_pyspark.show(2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dK7ZX0E9JyZA","outputId":"26af6aa2-b134-4dd4-bc68-20636a4a675e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+----------+----+------+------+---+------+----+------+\n","|total_bill| tip|   sex|smoker|day|  time|size|Visits|\n","+----------+----+------+------+---+------+----+------+\n","|     16.99|1.01|Female|    No|Sun|Dinner|   2|     7|\n","|     10.34|1.66|  Male|    No|Sun|Dinner|   3|     6|\n","+----------+----+------+------+---+------+----+------+\n","only showing top 2 rows\n","\n"]}]},{"cell_type":"code","source":["# Rename the columns\n","df_pyspark = df_pyspark.withColumnRenamed('sex','genre')\n","df_pyspark.show(2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GJoTZkK_KNwo","outputId":"9e9ed7a9-8066-45ef-feb7-1d2c840ec663"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+----------+----+------+------+---+------+----+------+--------------------------------------+\n","|total_bill| tip| genre|smoker|day|  time|size|Visits|Visited the restaurant more than twice|\n","+----------+----+------+------+---+------+----+------+--------------------------------------+\n","|     16.99|1.01|Female|    No|Sun|Dinner|   2|     7|                                  true|\n","|     10.34|1.66|  Male|    No|Sun|Dinner|   3|     6|                                  true|\n","+----------+----+------+------+---+------+----+------+--------------------------------------+\n","only showing top 2 rows\n","\n"]}]},{"cell_type":"markdown","source":["# Handling Missing Values"],"metadata":{"id":"3ey1qKcxLzrj"}},{"cell_type":"markdown","source":["## Let's create a Dataset"],"metadata":{"id":"UYfg4328KR30"}},{"cell_type":"code","source":["from pyspark.sql.functions import lit, col\n","from pyspark.sql.types import IntegerType, StringType, StructType, StructField"],"metadata":{"id":"Gi2HZeIAJ5yY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["schema = StructType([\n","    StructField(\"Type of Phenomenon\", StringType(), True),\n","    StructField(\"Year\", IntegerType(), True),\n","    StructField(\"Country\", StringType(), True),\n","    StructField(\"State\", StringType(), True),\n","    StructField(\"Number of Deaths\", IntegerType(), True)\n","])"],"metadata":{"id":"s9mJQ-yzMCaR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = [\n","    (\"Landslide\", 2020, \"USA\", \"California\", 15),\n","    (\"Debrisflow\", 2019, \"Japan\", \"Kyoto\", None),  # NaN value for 'Number of Deaths'\n","    (\"Flood\", 2021, \"India\", None, 120),             # NaN value for 'State'\n","    (\"Landslide\", None, \"Brazil\", \"Rio de Janeiro\", 8),  # NaN value for 'Year'\n","    (\"Flood\", 2022, \"China\", \"Guangdong\", None),       # NaN value for 'Number of Deaths'\n","    (\"Debrisflow\", 2018, \"Italy\", \"Rome\", 5)\n","]"],"metadata":{"id":"6qZTTMVMMHDO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_natural_disasters = spark.createDataFrame(data, schema=schema)\n","df_natural_disasters.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"34M3f5ZAMKHm","outputId":"5c76f36c-a012-4b84-824c-4d74c65fd610"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+------------------+----+-------+--------------+----------------+\n","|Type of Phenomenon|Year|Country|         State|Number of Deaths|\n","+------------------+----+-------+--------------+----------------+\n","|         Landslide|2020|    USA|    California|              15|\n","|        Debrisflow|2019|  Japan|         Kyoto|            NULL|\n","|             Flood|2021|  India|          NULL|             120|\n","|         Landslide|NULL| Brazil|Rio de Janeiro|               8|\n","|             Flood|2022|  China|     Guangdong|            NULL|\n","|        Debrisflow|2018|  Italy|          Rome|               5|\n","+------------------+----+-------+--------------+----------------+\n","\n"]}]},{"cell_type":"code","source":["## Drop NaN Values\n","print(\"DataFrame by default drop values\")\n","df_natural_disasters.na.drop().show()\n","\n","## If we use the parameters of the na property:\n","print(\"DataFrame with `how` and using `threshold` parameters\")\n","df_natural_disasters.na.drop(how=\"any\", thresh=2).show()\n","\n","print(\"DataFrame with `how` and `subset` parameters\")\n","df_natural_disasters.na.drop(how=\"any\", subset=[\"Number of Deaths\"]).show()"],"metadata":{"id":"RcLcsbYzMQLy","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3cf99b22-e39c-4a97-caec-af94155e0e0f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["DataFrame by default drop values\n","+------------------+----+-------+----------+----------------+\n","|Type of Phenomenon|Year|Country|     State|Number of Deaths|\n","+------------------+----+-------+----------+----------------+\n","|         Landslide|2020|    USA|California|              15|\n","|        Debrisflow|2018|  Italy|      Rome|               5|\n","+------------------+----+-------+----------+----------------+\n","\n","DataFrame with how = `any`\n","+------------------+----+-------+--------------+----------------+\n","|Type of Phenomenon|Year|Country|         State|Number of Deaths|\n","+------------------+----+-------+--------------+----------------+\n","|         Landslide|2020|    USA|    California|              15|\n","|        Debrisflow|2019|  Japan|         Kyoto|            NULL|\n","|             Flood|2021|  India|          NULL|             120|\n","|         Landslide|NULL| Brazil|Rio de Janeiro|               8|\n","|             Flood|2022|  China|     Guangdong|            NULL|\n","|        Debrisflow|2018|  Italy|          Rome|               5|\n","+------------------+----+-------+--------------+----------------+\n","\n","DataFrame with `how` and `subset` parameters\n","+------------------+----+-------+--------------+----------------+\n","|Type of Phenomenon|Year|Country|         State|Number of Deaths|\n","+------------------+----+-------+--------------+----------------+\n","|         Landslide|2020|    USA|    California|              15|\n","|             Flood|2021|  India|          NULL|             120|\n","|         Landslide|NULL| Brazil|Rio de Janeiro|               8|\n","|        Debrisflow|2018|  Italy|          Rome|               5|\n","+------------------+----+-------+--------------+----------------+\n","\n"]}]},{"cell_type":"code","source":["### Filling the missing values\n","\n","df_natural_disasters.na.fill(\"MISSING VALUES\").show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pa2zblLYK29z","outputId":"6a9db2ee-3dea-43a1-a4ed-5db7913c7c40"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+------------------+----+-------+--------------+----------------+\n","|Type of Phenomenon|Year|Country|         State|Number of Deaths|\n","+------------------+----+-------+--------------+----------------+\n","|         Landslide|2020|    USA|    California|              15|\n","|        Debrisflow|2019|  Japan|         Kyoto|            NULL|\n","|             Flood|2021|  India|MISSING VALUES|             120|\n","|         Landslide|NULL| Brazil|Rio de Janeiro|               8|\n","|             Flood|2022|  China|     Guangdong|            NULL|\n","|        Debrisflow|2018|  Italy|          Rome|               5|\n","+------------------+----+-------+--------------+----------------+\n","\n"]}]},{"cell_type":"code","source":["# To impute the values using advanced techniques\n","\n","from pyspark.ml.feature import Imputer\n","\n","imputer = Imputer(\n","    inputCols=[\"Number of Deaths\"],\n","    outputCols=[\"{}_imputed\".format(c) for c in [\"Number of Deaths\"]]\n",").setStrategy(\"mean\")"],"metadata":{"id":"2EBT-IJmMA3A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["imputer.fit(df_natural_disasters).transform(df_natural_disasters).show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"03lQGxL-Q-RY","outputId":"f3f196df-c06e-41ee-dacd-b8d6e943b935"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+------------------+----+-------+--------------+----------------+------------------------+\n","|Type of Phenomenon|Year|Country|         State|Number of Deaths|Number of Deaths_imputed|\n","+------------------+----+-------+--------------+----------------+------------------------+\n","|         Landslide|2020|    USA|    California|              15|                      15|\n","|        Debrisflow|2019|  Japan|         Kyoto|            NULL|                      37|\n","|             Flood|2021|  India|          NULL|             120|                     120|\n","|         Landslide|NULL| Brazil|Rio de Janeiro|               8|                       8|\n","|             Flood|2022|  China|     Guangdong|            NULL|                      37|\n","|        Debrisflow|2018|  Italy|          Rome|               5|                       5|\n","+------------------+----+-------+--------------+----------------+------------------------+\n","\n"]}]},{"cell_type":"markdown","source":["# Filter operation"],"metadata":{"id":"XfuqqEToSdeb"}},{"cell_type":"code","source":["# Using SQL syntaxis\n","df_natural_disasters.filter(\"`Number of Deaths`<10\").show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P5MEwtOCRCe0","outputId":"c9efcef6-4200-4f57-d32c-21c4196f8255"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+------------------+----+-------+--------------+----------------+\n","|Type of Phenomenon|Year|Country|         State|Number of Deaths|\n","+------------------+----+-------+--------------+----------------+\n","|         Landslide|NULL| Brazil|Rio de Janeiro|               8|\n","|        Debrisflow|2018|  Italy|          Rome|               5|\n","+------------------+----+-------+--------------+----------------+\n","\n"]}]},{"cell_type":"code","source":["# To be more specific\n","df_natural_disasters.filter(\"`Number of Deaths`<10\").select(['Country','Type of Phenomenon'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":87},"id":"ryZmenbUP2C1","outputId":"c10a2d89-9251-4d85-b07c-1dd77a387715"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["+-------+------------------+\n","|Country|Type of Phenomenon|\n","+-------+------------------+\n","| Brazil|         Landslide|\n","|  Italy|        Debrisflow|\n","+-------+------------------+"],"text/html":["<table border='1'>\n","<tr><th>Country</th><th>Type of Phenomenon</th></tr>\n","<tr><td>Brazil</td><td>Landslide</td></tr>\n","<tr><td>Italy</td><td>Debrisflow</td></tr>\n","</table>\n"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["# Using PySpark expressions\n","df_natural_disasters.filter(df_natural_disasters['Number of Deaths']<10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":87},"id":"d7KwEdrfQU9j","outputId":"9a0f0fd5-a3fe-420d-933e-8a17b9e09b47"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["+------------------+----+-------+--------------+----------------+\n","|Type of Phenomenon|Year|Country|         State|Number of Deaths|\n","+------------------+----+-------+--------------+----------------+\n","|         Landslide|NULL| Brazil|Rio de Janeiro|               8|\n","|        Debrisflow|2018|  Italy|          Rome|               5|\n","+------------------+----+-------+--------------+----------------+"],"text/html":["<table border='1'>\n","<tr><th>Type of Phenomenon</th><th>Year</th><th>Country</th><th>State</th><th>Number of Deaths</th></tr>\n","<tr><td>Landslide</td><td>NULL</td><td>Brazil</td><td>Rio de Janeiro</td><td>8</td></tr>\n","<tr><td>Debrisflow</td><td>2018</td><td>Italy</td><td>Rome</td><td>5</td></tr>\n","</table>\n"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["# Using different conditions\n","\n","df_natural_disasters.filter((df_natural_disasters['Number of Deaths']>10) &\n","                            (df_natural_disasters['Type of Phenomenon']== 'Landslide'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":64},"id":"lBGORVhpRQR8","outputId":"36d14587-fb00-417a-aaa3-68993831eec1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["+------------------+----+-------+----------+----------------+\n","|Type of Phenomenon|Year|Country|     State|Number of Deaths|\n","+------------------+----+-------+----------+----------------+\n","|         Landslide|2020|    USA|California|              15|\n","+------------------+----+-------+----------+----------------+"],"text/html":["<table border='1'>\n","<tr><th>Type of Phenomenon</th><th>Year</th><th>Country</th><th>State</th><th>Number of Deaths</th></tr>\n","<tr><td>Landslide</td><td>2020</td><td>USA</td><td>California</td><td>15</td></tr>\n","</table>\n"]},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","source":["# GroupBy and Aggregate Functions"],"metadata":{"id":"TvQKEbFBT37p"}},{"cell_type":"markdown","source":["Let's create more rows."],"metadata":{"id":"bn2-ypW2h6Rh"}},{"cell_type":"code","source":["from pyspark.sql import Row\n","\n","new_data = [\n","    Row(\"Earthquake\", 2023, \"Japan\", \"Kanto\", 150),\n","    Row(\"Flood\", 2022, \"India\", \"Kerala\", 40),\n","    Row(\"Hurricane\", 2021, \"USA\", \"Florida\", 85),\n","    Row(\"Tornado\", 2020, \"USA\", \"Oklahoma\", 30),\n","    Row(\"Volcano Eruption\", 2019, \"Indonesia\", \"Java\", 70),\n","    Row(\"Earthquake\", 2018, \"Mexico\", \"Oaxaca\", 90),\n","    Row(\"Flood\", 2023, \"Bangladesh\", \"Dhaka\", 60),\n","    Row(\"Tsunami\", 2022, \"Chile\", \"Valparaíso\", 200),\n","    Row(\"Drought\", 2021, \"Australia\", \"New South Wales\", 5),\n","    Row(\"Heatwave\", 2020, \"France\", \"Paris\", 110),\n","    Row(\"Earthquake\", 2019, \"Nepal\", \"Kathmandu\", 50),\n","    Row(\"Flood\", 2018, \"China\", \"Guangxi\", 45),\n","    Row(\"Hurricane\", 2023, \"Cuba\", \"Havana\", 95),\n","    Row(\"Wildfire\", 2022, \"Brazil\", \"Amazonas\", 25),\n","    Row(\"Tornado\", 2021, \"Canada\", \"Ontario\", 15),\n","    Row(\"Earthquake\", 2020, \"Turkey\", \"Izmir\", 65),\n","    Row(\"Flood\", 2019, \"Italy\", \"Veneto\", 20),\n","    Row(\"Hurricane\", 2018, \"Philippines\", \"Luzon\", 180),\n","    Row(\"Volcano Eruption\", 2023, \"Iceland\", \"Reykjavik\", 10),\n","    Row(\"Earthquake\", 2022, \"Peru\", \"Lima\", 45),\n","    Row(\"Flood\", 2021, \"Pakistan\", \"Sindh\", 80),\n","    Row(\"Hurricane\", 2020, \"Mexico\", \"Yucatan\", 50),\n","    Row(\"Tornado\", 2019, \"Argentina\", \"Santa Fe\", 35),\n","    Row(\"Volcano Eruption\", 2018, \"Italy\", \"Sicily\", 55),\n","    Row(\"Earthquake\", 2023, \"Turkey\", \"Ankara\", 130),\n","    Row(\"Flood\", 2022, \"Nigeria\", \"Lagos\", 60),\n","    Row(\"Hurricane\", 2021, \"Dominican Republic\", \"Santo Domingo\", 75),\n","    Row(\"Wildfire\", 2020, \"Australia\", \"Victoria\", 40),\n","    Row(\"Tornado\", 2019, \"USA\", \"Kansas\", 20),\n","    Row(\"Earthquake\", 2018, \"Greece\", \"Athens\", 85),\n","    Row(\"Flood\", 2023, \"Brazil\", \"Rio de Janeiro\", 70),\n","    Row(\"Earthquake\", 2022, \"Chile\", \"Santiago\", 120),\n","    Row(\"Hurricane\", 2021, \"Cuba\", \"Havana\", 95),\n","    Row(\"Tsunami\", 2020, \"Peru\", \"Lima\", 200),\n","    Row(\"Volcano Eruption\", 2019, \"Ecuador\", \"Quito\", 80),\n","    Row(\"Flood\", 2018, \"Argentina\", \"Buenos Aires\", 30),\n","    Row(\"Earthquake\", 2023, \"Mexico\", \"Mexico City\", 160),\n","    Row(\"Drought\", 2022, \"Venezuela\", \"Caracas\", 10),\n","    Row(\"Hurricane\", 2021, \"Dominican Republic\", \"Santo Domingo\", 75),\n","    Row(\"Wildfire\", 2020, \"Bolivia\", \"Santa Cruz\", 45),\n","    Row(\"Flood\", 2019, \"Colombia\", \"Bogotá\", 55),\n","    Row(\"Tornado\", 2018, \"Paraguay\", \"Asuncion\", 15),\n","    Row(\"Volcano Eruption\", 2023, \"Guatemala\", \"Antigua\", 100),\n","    Row(\"Flood\", 2022, \"Uruguay\", \"Montevideo\", 20),\n","    Row(\"Hurricane\", 2021, \"Honduras\", \"Tegucigalpa\", 65),\n","    Row(\"Tsunami\", 2020, \"El Salvador\", \"San Salvador\", 150),\n","    Row(\"Wildfire\", 2019, \"Panama\", \"Panama City\", 35),\n","    Row(\"Earthquake\", 2018, \"Nicaragua\", \"Managua\", 60),\n","    Row(\"Flood\", 2023, \"Costa Rica\", \"San Jose\", 25),\n","    Row(\"Volcano Eruption\", 2022, \"Nicaragua\", \"Leon\", 90),\n","    Row(\"Hurricane\", 2021, \"Belize\", \"Belize City\", 50),\n","    Row(\"Earthquake\", 2020, \"Haiti\", \"Port-au-Prince\", 180),\n","    Row(\"Flood\", 2019, \"Guatemala\", \"Guatemala City\", 40),\n","    Row(\"Wildfire\", 2018, \"Chile\", \"Valparaíso\", 30),\n","    Row(\"Tornado\", 2023, \"Argentina\", \"Cordoba\", 25),\n","    Row(\"Hurricane\", 2022, \"Mexico\", \"Cancun\", 80),\n","    Row(\"Flood\", 2021, \"Colombia\", \"Medellin\", 60),\n","    Row(\"Volcano Eruption\", 2020, \"Ecuador\", \"Guayaquil\", 70),\n","    Row(\"Earthquake\", 2019, \"Peru\", \"Arequipa\", 50),\n","    Row(\"Wildfire\", 2018, \"Brazil\", \"Sao Paulo\", 20),\n","    Row(\"Tsunami\", 2023, \"Chile\", \"La Serena\", 110),\n","    Row(\"Drought\", 2022, \"Mexico\", \"Chihuahua\", 15),\n","    Row(\"Flood\", 2021, \"Venezuela\", \"Maracaibo\", 45),\n","    Row(\"Hurricane\", 2020, \"Nicaragua\", \"Bluefields\", 85),\n","    Row(\"Tornado\", 2019, \"Brazil\", \"Porto Alegre\", 35)\n","]"],"metadata":{"id":"p92upO4Xh980"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Cretae a DataFrame with the new rows\n","\n","new_df = spark.createDataFrame(new_data, schema=[\"Type of Phenomenon\", \"Year\", \"Country\", \"State\", \"Number of Deaths\"])\n","\n","df_natural_disasters = df_natural_disasters.union(new_df)"],"metadata":{"id":"oh3IvkKPjYM6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def pyspark(df):\n","    num_rows = df.count()\n","    num_cols = len(df.columns)\n","    print(f\"Number of rows: {num_rows}\")\n","    print(f\"Number of columns: {num_cols}\")\n","    return"],"metadata":{"id":"6Il9xSq_jYQQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pyspark(df_natural_disasters)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b3W0Q1jKjYTf","outputId":"d17cd33e-fb1b-4024-c84d-bf8238d7bd16"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of rows: 71\n","Number of columns: 5\n"]}]},{"cell_type":"code","source":["df_natural_disasters.columns"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FR0EAHs9SOTi","outputId":"caed5c08-6050-42fe-9df4-a100b69e682a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Type of Phenomenon', 'Year', 'Country', 'State', 'Number of Deaths']"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["df_natural_disasters.dtypes"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YsrDJrxzbD5d","outputId":"a70c48cb-2381-469c-cc90-5d795406bbd5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('Type of Phenomenon', 'string'),\n"," ('Year', 'int'),\n"," ('Country', 'string'),\n"," ('State', 'string'),\n"," ('Number of Deaths', 'int')]"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["# GroupBy\n","\n","df_natural_disasters.groupBy('Country').sum('Number of Deaths').show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uWAx-8t_bJEb","outputId":"dc0ae4c2-d7c0-4c0a-b64c-6261bbeb369a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----------+---------------------+\n","|    Country|sum(Number of Deaths)|\n","+-----------+---------------------+\n","|      India|                  160|\n","|        USA|                  150|\n","|      Japan|                  150|\n","|      China|                   45|\n","|      Italy|                   80|\n","|     Brazil|                  158|\n","|Philippines|                  180|\n","|     Turkey|                  195|\n","|     France|                  110|\n","|     Greece|                   85|\n","|  Argentina|                   90|\n","|       Peru|                  295|\n","|      Chile|                  460|\n","|    Nigeria|                   60|\n","|       Cuba|                  190|\n","| Bangladesh|                   60|\n","|    Iceland|                   10|\n","|     Mexico|                  395|\n","|  Indonesia|                   70|\n","|     Canada|                   15|\n","+-----------+---------------------+\n","only showing top 20 rows\n","\n"]}]},{"cell_type":"code","source":["df_natural_disasters.groupBy('Country').count().orderBy('count', ascending=False).show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fAhTJJM9ldYo","outputId":"99a4d1ff-5973-4e3a-8d33-5a53b008d37a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+------------------+-----+\n","|           Country|count|\n","+------------------+-----+\n","|            Brazil|    5|\n","|            Mexico|    5|\n","|               USA|    4|\n","|             Chile|    4|\n","|             Italy|    3|\n","|         Argentina|    3|\n","|              Peru|    3|\n","|         Nicaragua|    3|\n","|             India|    2|\n","|             Japan|    2|\n","|             China|    2|\n","|            Turkey|    2|\n","|              Cuba|    2|\n","|Dominican Republic|    2|\n","|         Australia|    2|\n","|           Ecuador|    2|\n","|         Venezuela|    2|\n","|         Guatemala|    2|\n","|          Colombia|    2|\n","|       Philippines|    1|\n","+------------------+-----+\n","only showing top 20 rows\n","\n"]}]},{"cell_type":"code","source":["df_natural_disasters.agg({'Number of Deaths': 'sum'}).show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PIO2IEUomdIi","outputId":"6d45039b-332d-4177-c964-800bce2501d0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+---------------------+\n","|sum(Number of Deaths)|\n","+---------------------+\n","|                 4563|\n","+---------------------+\n","\n"]}]},{"cell_type":"code","source":["# To export the data in parquet format\n","df_natural_disasters.write.parquet(\"/content/drive/MyDrive/big-data/pyspark-getting-started/data/df_natural_disasters.parquet\")"],"metadata":{"id":"k5XMXQVVt-zI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# MLlib"],"metadata":{"id":"Pl3lOqajqhcP"}},{"cell_type":"markdown","source":["[MLlib (DataFrame-based)](https://spark.apache.org/docs/latest/api/python/reference/pyspark.ml.html)\n","\n","[Machine Learning Library (MLlib) Guide](https://spark.apache.org/docs/latest/ml-guide)"],"metadata":{"id":"dLGs0Ui3qsIu"}},{"cell_type":"markdown","source":["## Create a Dataframe"],"metadata":{"id":"i_nidYCAHv1u"}},{"cell_type":"code","source":["import random\n","\n","# To make reproducible the results\n","random.seed(30)\n","\n","# Create a couple functions to generate random data\n","def random_int(lower, upper):\n","    return random.randint(lower, upper)\n","\n","def random_float(lower, upper):\n","    return round(random.uniform(lower, upper), 2)\n","\n","# American States\n","us_states = [\n","    \"California\", \"Texas\", \"Florida\", \"New York\", \"Illinois\"\n","]\n","\n","\n","# Generar datos ficticios para 100 registros\n","data = [\n","    (random_int(18, 80),  # Age\n","     random_int(0, 20),   # Years_with_Company\n","     random_int(0, 100),  # Health_Score\n","     random_int(0, 10),   # Number_of_Claims\n","     random_float(1000, 20000),  # Income\n","     random.choice(us_states)  # State\n","    )\n","    for _ in range(100) # Just 100 rows for this example\n","]\n","\n","# Define the schema\n","columns = [\"Age\", \"Years_with_Company\", \"Health_Score\", \"Number_of_Claims\", \"Income\", \"State\"]\n","\n","# Crear el DataFrame\n","df_health_income = spark.createDataFrame(data, columns)\n","\n","# To save the dataframe\n","df_health_income.write.mode(\"overwrite\").parquet(\"/content/drive/MyDrive/big-data/pyspark-getting-started/data/df_health_income.parquet\")\n","\n","df_health_income.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_Z2sj_KYHxqV","executionInfo":{"status":"ok","timestamp":1725203360707,"user_tz":300,"elapsed":1364,"user":{"displayName":"DAVID PALACIO J.","userId":"15633330119049240004"}},"outputId":"d69bcc3c-4647-4299-ddca-368f7f2796d1"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+------------------+------------+----------------+--------+----------+\n","|Age|Years_with_Company|Health_Score|Number_of_Claims|  Income|     State|\n","+---+------------------+------------+----------------+--------+----------+\n","| 52|                 9|          78|               0|12815.64|     Texas|\n","| 80|                 8|           6|               6|  8147.4|     Texas|\n","| 23|                14|           0|               8|19858.87|     Texas|\n","| 19|                 2|          20|               9|11085.66|  New York|\n","| 80|                20|          44|               8|17759.33|  New York|\n","| 19|                 7|          72|              10|  6316.2|  New York|\n","| 54|                 3|          86|               4|13135.73|California|\n","| 57|                15|          39|               2|16757.55|   Florida|\n","| 34|                16|          10|               2| 5621.25|   Florida|\n","| 25|                 0|          52|               8| 6718.14|  Illinois|\n","| 59|                 5|          60|              10| 7293.62|  New York|\n","| 73|                10|          66|               5| 2296.86|California|\n","| 48|                15|          33|               7| 5819.71|   Florida|\n","| 75|                 5|          87|               2|16732.21|  Illinois|\n","| 36|                13|          85|               3|18760.12|  New York|\n","| 41|                13|          94|               9| 6689.45|  New York|\n","| 60|                20|          48|               4|16148.78|  New York|\n","| 41|                 9|          66|               2|14703.62|   Florida|\n","| 51|                10|           2|               3| 3271.23|   Florida|\n","| 53|                 1|          74|               8|18484.95|  New York|\n","+---+------------------+------------+----------------+--------+----------+\n","only showing top 20 rows\n","\n"]}]},{"cell_type":"markdown","source":["## Upload DataFrame"],"metadata":{"id":"ZBfpTrdpHzNE"}},{"cell_type":"code","source":["df_health_income = spark.read.parquet(\"/content/drive/MyDrive/big-data/pyspark-getting-started/data/df_health_income.parquet\")"],"metadata":{"id":"7478nj7DqrWE","executionInfo":{"status":"ok","timestamp":1725203367319,"user_tz":300,"elapsed":243,"user":{"displayName":"DAVID PALACIO J.","userId":"15633330119049240004"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["df_health_income.show(2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1a44B9CNHVh8","executionInfo":{"status":"ok","timestamp":1725203368760,"user_tz":300,"elapsed":494,"user":{"displayName":"DAVID PALACIO J.","userId":"15633330119049240004"}},"outputId":"a6a3be15-7184-48ff-92e9-f932484fcbab"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+------------------+------------+----------------+-------+--------+\n","|Age|Years_with_Company|Health_Score|Number_of_Claims| Income|   State|\n","+---+------------------+------------+----------------+-------+--------+\n","| 41|                11|          52|               1|1844.99|Illinois|\n","| 29|                15|           0|               5|4032.25| Florida|\n","+---+------------------+------------+----------------+-------+--------+\n","only showing top 2 rows\n","\n"]}]},{"cell_type":"markdown","source":["## Feature Engineering"],"metadata":{"id":"z4vvmidHH6We"}},{"cell_type":"markdown","source":["In Spark, we need first use\n","`StringIndexer` before to apply `OneHotEncoder` because it doesn't work directly in numeric columns."],"metadata":{"id":"tdCKwrJyJuUj"}},{"cell_type":"code","source":["# `StringIndexer`\n","\n","from pyspark.ml.feature import StringIndexer\n","\n","# State --> Numbers\n","indexer = StringIndexer(inputCol=\"State\", outputCol=\"StateIndex\")\n","df_indexed = indexer.fit(df_health_income).transform(df_health_income)\n","\n","df_indexed.show(1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y6oDIJ1uJ6V8","executionInfo":{"status":"ok","timestamp":1725203779957,"user_tz":300,"elapsed":4225,"user":{"displayName":"DAVID PALACIO J.","userId":"15633330119049240004"}},"outputId":"df675852-50f6-4867-bbfe-b93824935805"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+------------------+------------+----------------+-------+--------+----------+\n","|Age|Years_with_Company|Health_Score|Number_of_Claims| Income|   State|StateIndex|\n","+---+------------------+------------+----------------+-------+--------+----------+\n","| 41|                11|          52|               1|1844.99|Illinois|       4.0|\n","+---+------------------+------------+----------------+-------+--------+----------+\n","only showing top 1 row\n","\n"]}]},{"cell_type":"code","source":["# First, we use one-hot encoding for the categorical variables\n","\n","from pyspark.ml.feature import OneHotEncoder\n","\n","# Aplicar One-Hot Encoding a la columna indexada \"StateIndex\"\n","encoder = OneHotEncoder(inputCol=\"StateIndex\", outputCol=\"StateVector\")\n","df_encoded = encoder.fit(df_indexed).transform(df_indexed)\n","\n","df_encoded.show(1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oyiVeKUgJRY2","executionInfo":{"status":"ok","timestamp":1725203840568,"user_tz":300,"elapsed":1017,"user":{"displayName":"DAVID PALACIO J.","userId":"15633330119049240004"}},"outputId":"63c35eed-ae58-4243-9f56-ecff53b9f764"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+------------------+------------+----------------+-------+--------+----------+-----------+\n","|Age|Years_with_Company|Health_Score|Number_of_Claims| Income|   State|StateIndex|StateVector|\n","+---+------------------+------------+----------------+-------+--------+----------+-----------+\n","| 41|                11|          52|               1|1844.99|Illinois|       4.0|  (4,[],[])|\n","+---+------------------+------------+----------------+-------+--------+----------+-----------+\n","only showing top 1 row\n","\n"]}]},{"cell_type":"markdown","source":["Now, we are going to use [VectorAssembler](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.VectorAssembler.html)"],"metadata":{"id":"sUObKmgWKcDz"}},{"cell_type":"code","source":["# [Years_with_Company, Health_Score, Number_of_Claims] ---> new feature ---> independent feature"],"metadata":{"id":"Nt0DM_lEHezU","executionInfo":{"status":"ok","timestamp":1725203400027,"user_tz":300,"elapsed":236,"user":{"displayName":"DAVID PALACIO J.","userId":"15633330119049240004"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler\n","featureassembler = VectorAssembler(inputCols=[\"Age\", \"Years_with_Company\", \"Health_Score\", \"Number_of_Claims\", \"StateIndex\"], outputCol=\"Independent Features\")"],"metadata":{"id":"7pn8ez_yIsMs","executionInfo":{"status":"ok","timestamp":1725203907724,"user_tz":300,"elapsed":263,"user":{"displayName":"DAVID PALACIO J.","userId":"15633330119049240004"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["# Use transforme over the dataset\n","\n","output = featureassembler.transform(df_indexed)\n","\n","output.show(2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F1D-BbNJIxjs","executionInfo":{"status":"ok","timestamp":1725204299128,"user_tz":300,"elapsed":509,"user":{"displayName":"DAVID PALACIO J.","userId":"15633330119049240004"}},"outputId":"887eeb8a-9542-462f-96b6-e02b64986640"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+------------------+------------+----------------+-------+--------+----------+--------------------+\n","|Age|Years_with_Company|Health_Score|Number_of_Claims| Income|   State|StateIndex|Independent Features|\n","+---+------------------+------------+----------------+-------+--------+----------+--------------------+\n","| 41|                11|          52|               1|1844.99|Illinois|       4.0|[41.0,11.0,52.0,1...|\n","| 29|                15|           0|               5|4032.25| Florida|       1.0|[29.0,15.0,0.0,5....|\n","+---+------------------+------------+----------------+-------+--------+----------+--------------------+\n","only showing top 2 rows\n","\n"]}]},{"cell_type":"code","source":["# Show a sample row from the DataFrame\n","sample_row = output.select(\"Independent Features\").head(1)[0]\n","\n","# Access the vector and print its size\n","vector_size = len(sample_row[\"Independent Features\"])\n","print(f\"Vector size: {vector_size}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l5T2MXkPM07C","executionInfo":{"status":"ok","timestamp":1725204496347,"user_tz":300,"elapsed":520,"user":{"displayName":"DAVID PALACIO J.","userId":"15633330119049240004"}},"outputId":"b4bfeb9d-0fe9-4f1a-8e07-ff248b8fa40a"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["Vector size: 5\n"]}]},{"cell_type":"markdown","source":["The column called `Independent Features` have a size of 5, which is correct accordind to the problem: `Age`, `Years_with_Company`, `Health_Score`, `Number_of_Claims`, `State`."],"metadata":{"id":"H-OFR5fJI600"}},{"cell_type":"code","source":["finalized_data = output.select(\"Independent Features\", \"Income\")\n","finalized_data.show(2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qt73SjP0I4yd","executionInfo":{"status":"ok","timestamp":1725204309542,"user_tz":300,"elapsed":487,"user":{"displayName":"DAVID PALACIO J.","userId":"15633330119049240004"}},"outputId":"3715a661-c2a5-471c-a5d5-9f68f75c15d1"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["+--------------------+-------+\n","|Independent Features| Income|\n","+--------------------+-------+\n","|[41.0,11.0,52.0,1...|1844.99|\n","|[29.0,15.0,0.0,5....|4032.25|\n","+--------------------+-------+\n","only showing top 2 rows\n","\n"]}]},{"cell_type":"markdown","source":["## Training the model"],"metadata":{"id":"ymQcQZMNNUnZ"}},{"cell_type":"code","source":["from pyspark.ml.regression import LinearRegression\n","\n","# Divide train and test\n","train_data, test_data = finalized_data.randomSplit([0.7, 0.30])\n","regressor=LinearRegression(featuresCol='Independent Features', labelCol='Income')\n","regressor=regressor.fit(train_data)"],"metadata":{"id":"JNhPcGDALnQ8","executionInfo":{"status":"ok","timestamp":1725204873450,"user_tz":300,"elapsed":2849,"user":{"displayName":"DAVID PALACIO J.","userId":"15633330119049240004"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["## Coefficients\n","\n","regressor.coefficients"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YFOCFXlpOYjX","executionInfo":{"status":"ok","timestamp":1725204921761,"user_tz":300,"elapsed":241,"user":{"displayName":"DAVID PALACIO J.","userId":"15633330119049240004"}},"outputId":"5b132b9a-6c57-475d-878f-c2268cc3e5c2"},"execution_count":40,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DenseVector([-18.0384, 51.7791, 18.3594, 240.9742, -262.8451])"]},"metadata":{},"execution_count":40}]},{"cell_type":"code","source":["## Intercepts\n","\n","regressor.intercept"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3UoFzWAtOfrZ","executionInfo":{"status":"ok","timestamp":1725204942665,"user_tz":300,"elapsed":230,"user":{"displayName":"DAVID PALACIO J.","userId":"15633330119049240004"}},"outputId":"f9c5a0a7-222f-41cd-fe44-666762042215"},"execution_count":41,"outputs":[{"output_type":"execute_result","data":{"text/plain":["7986.137690483762"]},"metadata":{},"execution_count":41}]},{"cell_type":"code","source":["## Prediction\n","\n","pred_results = regressor.evaluate(test_data)"],"metadata":{"id":"Pt-Lk_D8OlcX","executionInfo":{"status":"ok","timestamp":1725204962043,"user_tz":300,"elapsed":491,"user":{"displayName":"DAVID PALACIO J.","userId":"15633330119049240004"}}},"execution_count":42,"outputs":[]},{"cell_type":"code","source":["pred_results.predictions.show(5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5YsvK6PKOpdp","executionInfo":{"status":"ok","timestamp":1725204979203,"user_tz":300,"elapsed":763,"user":{"displayName":"DAVID PALACIO J.","userId":"15633330119049240004"}},"outputId":"b6edba8e-8269-4c11-9468-d69abf5cf3b5"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["+--------------------+--------+------------------+\n","|Independent Features|  Income|        prediction|\n","+--------------------+--------+------------------+\n","|[19.0,20.0,95.0,4...|  9461.1|10861.335413583201|\n","|[23.0,20.0,16.0,4...|19575.98| 9338.792051157498|\n","|[24.0,3.0,7.0,6.0...| 2802.26| 8757.222632133045|\n","|[26.0,5.0,11.0,0....|14033.48|  7977.98657625993|\n","|[30.0,7.0,28.0,6....| 7670.49| 9504.500065982602|\n","+--------------------+--------+------------------+\n","only showing top 5 rows\n","\n"]}]},{"cell_type":"code","source":["## MAE and MSE\n","\n","pred_results.meanAbsoluteError, pred_results.meanSquaredError"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BqOE5E2xOtkk","executionInfo":{"status":"ok","timestamp":1725205020286,"user_tz":300,"elapsed":285,"user":{"displayName":"DAVID PALACIO J.","userId":"15633330119049240004"}},"outputId":"243cdd3f-1a42-45e8-8c91-a9035bb2c4a7"},"execution_count":44,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(4185.992675400528, 24460004.546018045)"]},"metadata":{},"execution_count":44}]}]}